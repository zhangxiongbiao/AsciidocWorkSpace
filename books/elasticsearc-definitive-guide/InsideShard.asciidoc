= 深入理解分片
:imagesdir: images

在 <<Life Inside a Cluster>> 中，我们介绍了分片，也说过分片是一个较底层的工作单元。但是分片到底是如何工作的呢？

本章主要解决以下问题：

* 为什么搜索是 *近* 实时的？
* 为什么文档的 CRUD 操作是近实时的？
* _ES_ 是如何保证你所做的更改操作持久化的，而且在宕机情况下这些更改操作都不会丢失。
* 为什么删除操作并不会立即清除磁盘空间。
* `refresh` 、`flush` 、 `optimize` 这些 _API_ 操作都是干嘛的，你又何时需要用呢?


下面，将讲解如何会出现这些问题，并且又是如何解决这些问题的。

== 使文本可搜索

在 _ES_ 中首先需要解决的是使文本可以被搜索。在传统数据中，每一个字段存储一个值，但是这并不适合全文搜索。如果说在一个字段的文本值的每一个词或字都需要可搜索的话，那么意味着这个字段需要对应多个值。

这种一个字段多个值的需求，正是 _倒排索引_ 需要解决的问题。倒排索引以字对应文档的形式存储。

[source,js]
----
Term  | Doc 1 | Doc 2 | Doc 3 | ...
------------------------------------
brown |   X   |       |  X    | ...
fox   |   X   |   X   |  X    | ...
quick |   X   |   X   |       | ...
the   |   X   |       |  X    | ...
----

[TIP]
====
_ES_ 中每一个被索引的字段，都有其对应的倒排索引。
====

倒排索引存储了比包含了一个特定term的文档列表多地多的信息。它可能存储包含每个term的文档数量，一个term出现在指定文档中的频次，每个文档中term的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息让Elasticsearch知道哪些term更重要，哪些文档更重要，也就是相关性。

需要意识到，为了实现倒排索引预期的功能，它必须要知道集合中所有的文档。

在全文检索的早些时候，会为整个文档集合建立一个大索引，并且写入磁盘。只有新的索引准备好了，它就会替代旧的索引，最近的修改才可以被检索。意思就是你哪怕做了一个小的更改（添加了一个文档），就必须重新准备一个新的大索引用来替代旧的索引。

=== 不可变性

倒排索引写入到磁盘中后就是不可变的。这种不可变性有以下几大好处：

. 这个时候不需要锁，因为你压根就不能修改倒排索引，所以自然就不存在会有多个线程并发修改了。
. 一旦索引被读入文件系统的缓存（物理内存，可自行百度Linux文件系统缓存），它就一直在那儿，因为不会改变。只要文件系统缓存有足够的空间，大部分的读会直接访问内存而不是磁盘。这有助于性能提升。
. 在索引的声明周期内，所有的其他缓存（比如过滤查询的缓存）都可用。它们不需要在每次数据变化了都重建，因为数据不会变。
. 写入单个大的倒排索引，可以压缩数据，减少磁盘IO和需要缓存索引的内存大小。


当然，不可变的索引有它的缺点，首先是它不可变！你不能改变它。如果想要搜索一个新文档，必须重建整个索引。这不仅严重限制了一个索引所能装下的数据，还限制了一个索引可以被更新的频次。


== 动态更新索引

在上面提到了倒排索引的不可变性，这也就意味着新添加一个文档就需要重建整个倒排索引。

如何在即保持倒排索引的不可变性的好处，又能更新倒排索引呢？答案就是使用多个倒排索引。

相比每次做了一点修改就去重建整个倒排索引，我们可以额外增加倒排索引来反映近期的修改变化。然后查询的时候按照顺序查询，从最老的倒排索引到最新的倒排索引依次进行查询，然后把结果合并（新的覆盖旧的结果）。

在 _Lucene_ 中，提出了一个概念 “ _per-segment search_ ” ，每一个段（物理文件）都是一个倒排索引，而且在 _Lucene_ 中 *_index_* 这个词用来表示 *一组段* 加 *一个提交点* 。

.Index与Shard
****
我们在 _ES_ 中称呼的 *_Shard_ 就是 _Lucene_ 中的 _index_* 。而 _ES_ 中的 _index_ 则是一组 _Shard_ 的集合。
****

[TIP]
====
提交点是一个记录所有已知段的文件
====


image::elas_1101.png[]

image::elas_1102.png[]

image::elas_1103.png[]

上面整个过程，即 _per-segment search_

. 新文档首先进入索引缓冲区
. 不时，缓冲区内容提交
.. 一个新段（倒排索引），写入到了硬盘
.. 新的段名称记录到提交点中，提交点内容更新到硬盘
.. 硬盘确保是 fsync’ed （  http://blog.csdn.net/cywosp/article/details/8767327[fsync] ），即保存文件系统缓存中的内容全部写入到了硬盘，物理写入。
. 刚添加的新段被打开（文件打开），并使其中的文档可以被搜索
. 内存中的索引缓冲区被清空，等待新的修改进入

当查询请求到达分片上时，其下的所有段都依次按新老顺序查询。这种情况下，添加新的文档就比较简单了。

== 删除与更新

因为段（倒排索引）是不可变的，所以也就是旧的文档不能从段中删除。每一个提交点都包含一个 *.del* 文件，这个文件用来记录段上的哪些文档已经被删除了。

当一个文档被标记为删除时，其实只是在 _.del_ 文件中做了个记录。这时，被标记为删除的文档其实还是被查询匹配到的，只不过在返回查询结果前会将被标记为删除的文档给过滤掉。

更新操作也是类似的，当一个文档更新时，旧的文档被标记为删除，然后新的文档被添加到新段中。当然这多个不同版本的文档可能都会被查询匹配到，不过
在返回结果前旧的版本的文档会被过滤掉。


在下面的 <<合并段>> 章节，将介绍被标记为删除的文档如何从文件系统中真正删除。

== 近实时搜索

在文档被添加到新段中并可被搜索时，这中间的延迟还是很明显的。尽管可以快到1分钟内就可以搜索了，但是还是不够快。

这里主要的瓶颈在于硬盘，即硬盘写性能差了。提交一个新段到硬盘需要 `fsync` 操作来确保段被真正的物理写入到硬盘了。但是 `fsync` 操作代价比较高，所以不能每有一个文档写入时就执行 `fsync` 。


在 _ES_ 中，使用文件系统缓存来避免频繁地执行 `fsync` 操作。文件从索引缓冲区写入到新段中后，新段是先写入到文件系统缓存，稍候再会写入硬盘。不过一但文件在文件系统缓存中时，它就已经可以被打开并可读了，这就意味着这时这个新段就已经可以被搜索了。

.已写入新段（在文件系统缓存中），此时未提交，但是已经可以搜索了。
image::elas_1105.png[]

_Lucene_ 允许在未执行一个完整的段提交时，新段在文件系统中就可以被打开并搜索。



=== refresh API

在 _ES_ 中，上面提到的这个 *写及打开新段使可搜索* 的操作称为 _refresh_ 。默认情况下，每一个分片都是默认 _1s_ 刷新一次。就也就是上面说的 _ES_ 是近实时搜索了，文档写入后并不会立即可以被搜索，但是 _1s_ 内就可以了。


上面自动刷新的操作，可以通过 _refresh api_ 手动执行。

[source,js]
----
POST /_refresh  // <1>
POST /blogs/_refresh // <2>
----
<1> 刷新所有索引
<2> 只刷新 _blogs_ 索引


[TIP]
====
尽管 _refresh_ 操作比起 *提交段* （指写新的提交点，段物理写入硬盘） 操作要耗费性能小得多。 如果你在生产系统下，频繁地写时你又每隔 _1s_ 刷新的话，还是会浪费不少的性能。此时如果并不那么严格要求立即可搜索的话，可以将 _refresh_ 的间隔时间调大一些。
====

通过指定 `refresh_interval` 来设置刷新频率。

[source,js]
----
PUT /my_logs
{
  "settings": {
    "refresh_interval": "30s" // <1>
  }
}
----
<1> 设置该索引每隔30s刷新一次

`refresh_interval` 可以在一个已经运行的索引上动态更新。

[source,js]
----
PUT /my_logs/_settings
{ "refresh_interval": -1 } // <1>

PUT /my_logs/_settings
{ "refresh_interval": "1s" } // <2>
----
<1> 禁用刷新。通过全量写数据或迁移数据时使用。
<2> 设置为1s刷新。

[CAUTION]
====
这里设置的 `refresh_interval` 都最好把单位带上，如 `1s` 或 `1m` 。如果不指定单位的话，默认单位为 `ms` ，如果你设置个1ms的话，那么你的集群在频繁写数据时绝对会跪了。
====

== 使数据持久化

如果说没有每次执行 `fsync` 操作来确保将数据完全写入硬盘的话，那么当系统断电时，那些在文件系统缓存（实际为物理内存）中的数据可能会丢失。


我们说过一次全提交同步段到磁盘，写提交点，这会列出所有的已知的段。在重启，或重新打开索引时，ES使用这次提交点决定哪些段属于当前的分片。

当我们通过每秒的刷新获得近实时的搜索，我们依然需要定时地执行全提交确保能从失败中恢复。但是提交之间的文档怎么办？我们也不想丢失它们。

ES增加了事务日志（translog），来记录每次操作。有了事务日志，过程现在如下：

. 当一个文档被索引，它被加入到内存缓存，同时加到事务日志。
+
image::elas_1106.png[]
. refresh使得分片的进入如下图描述的状态。每秒分片都进行refeash：
.. 内存缓冲区的文档写入到段中，但没有fsync。
.. 段被打开，使得新的文档可以搜索。
.. 缓存被清除
+
.经过刷新后，索引缓存被清空了，但是事务日志没有被清除
image::elas_1107.png[]

. 随着更多的文档加入到缓存区，写入日志，这个过程会继续
+
image::elas_1108.png[]

. 不时地，比如日志很大了，新的日志会创建，会进行一次段提交（写盘）
.. 内存缓存区的所有文档会写入到新段中。
.. 清除缓存
.. 一个提交点写入硬盘
.. 文件系统缓存通过fsync操作flush到硬盘
.. 事务日志被清除


事务日志记录了没有flush到硬盘的所有操作。当故障重启后，ES会用最近一次提交点从硬盘恢复所有已知的段，并且从日志里恢复所有的操作。

事务日志还用来提供实时的CRUD操作。当你尝试用ID进行CRUD时，它在检索相关段内的文档前会首先检查日志最新的改动。这意味着ES可以实时地获取文档的最新版本。

.flush过后，段被全提交，事务日志清除
image::elas_1109.png[]

=== flush api
在 _ES_ 中，执行提交段并截断事务日志的操作被称为 _flush_ 。 *分片默认是每隔30分钟 flush 一次* 或者 事务日志太大时 flush 。具体可以看 translog 的文档来控制这个频率。

[source,js]
----
POST /blogs/_flush // <1>

POST /_flush?wait_for_ongoing // <2>
----
<1> flush 索引 blogs
<2> flush 所有索引，并且一直等待直到所有索引全部刷新完成

一般情况下，你不需要自己来手动 _flush_ 。不过，你可以在重启一个节点，或者关闭某个索引时，执行这个操作（不然，节点重启后得读事务日志恢复）。

当 _ES_ 恢复节点或重新打开索引时，都会恢复事务日志中记录的操作。所以事务日志越小，这个恢复速度越快。

.translog有多安全？
****
使用事务日志的目的是保证所做的操作（修改）不会丢失。

写入到文件中的数据，如果没有 `fsync` 的话，在系统重启后就会丢失。默认情况下，translog是每隔5s `fsync`一次，并且在每一次请求（如index,update,delete,bulk）完成后也会执行，并且是在主、备分片上都执行。这也就是说，你做了一个修改操作后，必须要等到事务日志在主备分片上都执行 `fsync` 后才会收到响应。

在每一个请求执行完后执行一个 `fsync`还是比较耗性能的，尽管在实践后相对来说还是比较小的性能损耗（尤其是 _bulk_ 请求）。

但是在一些大容量卷中，如果可以容忍有几秒的数据丢失，那么可以设置 `fsync` 操作异步执行。

[source,js]
----
PUT /my_index/_settings
{
    "index.translog.durability": "async",
    "index.translog.sync_interval": "5s"
}
----

上面这个配置是可以随时动态更新的。如果你决定了使用 `async` 方式来写事务日志的话，那么你得承担 `sync_interval` 时间内的数据丢失情况（系统断电等情况时）。

****

== 合并段

通过每秒自动刷新创建新的段，用不了多久段的数量就爆炸了。有太多的段是一个问题。每个段消费文件句柄，内存，cpu资源。更重要的是，每次搜索请求都需要依次检查每个段。段越多，查询越慢。

ES通过后台合并段解决这个问题。小段被合并成大段，再合并成更大的段。

这是旧的文档从文件系统删除的时候。旧的段不会再复制到更大的新段中。

这个过程你不必做什么。当你在索引和搜索时ES会自动处理。这个过程如图：两个提交的段和一个未提交的段合并为了一个更大的段所示：

. 索引过程中，refresh会创建新的段，并打开它。

. 合并过程会在后台选择一些小的段合并成大的段，这个过程不会中断索引和搜索。
+
.两个提交的段和一个未提交的段合并为了一个更大的段
image::elas_1110.png[]
. 下图描述了合并后的操作：
.. 新的段flush到了硬盘。
.. 新的提交点写入新的段，排除旧的段。
.. 新的段打开供搜索。
.. 旧的段被删除。
+
.段合并完后，旧的段被删除
image::elas_1111.png[]

合并大的段会消耗很多IO和CPU，如果不检查会影响到搜素性能。默认情况下，ES会限制合并过程，这样搜索就可以有足够的资源进行。


=== optimize api

_optimize api_ 即强制合并，它用于指示一个 _shard_ 将段合并到指定的 max_num_segments (最大段数)。这是为了减少段的数量（通常会减少到1）达到提高搜索性能的目的。

[WARNING]
====
不要在动态的索引（正在活跃更新）上使用optimize API。后台的合并处理已经做的很好了，优化命令会阻碍它的工作。不要干涉！
====

在特定的环境下，optimize API是有用的。典型的场景是记录日志，这中情况下日志是按照每天，周，月存入索引。旧的索引一般是只可读的，它们是不可能修改的。
这种情况下，把每个索引的段降至1是有效的。搜索过程就会用到更少的资源，性能更好:

[source,js]
----
POST /logstash-2014-10/_optimize?max_num_segments=1 // <1>
----
<1> 把索引中的每个分片下的段都合并成一个段


[WARNING]
====
使用 _optimize_ 触发的合并操作，一点都不会被抑制。它极有可能会将你的节点上的所有 I/O 资源占用完，这也就可能会导致你的集群无响应。
如果你打算使用 _optimize_ ，建议将分片全部移至一个节点然后单独跑。
====
