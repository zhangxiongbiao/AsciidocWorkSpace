= 映射与分析
:imagesdir: images

我们在处理数据时，有时可能会注意到一些奇怪的现象。为什么我的索引里面有12条数据，并且其中只有一条数据的 _date_ 是 2014-09-15 ，然而执行以下几种查询后的结果却是：

[source,js]
----
GET /us,gb/_search?q=2014             # 12 results

GET /us,gb/_search?q=2014-09-15       # 12 results !

GET /us,gb/_search?q=date:2014-09-15  # 1  result

GET /us,gb/_search?q=date:2014        # 0  results !
----
上面可以看到，为什么用 2014-09-15 检索 `_all` 字段时返回了12条数据，而用 2014 检索 date 字段却没有结果返回？

因为存储数据到 `_all` 与 date 字段时的方式是不同的。

[TIP]
====
`_all` 为字符串类型，且是所有字段合集，而 date 为日期类型。
====

查看一下 _tweet_ 的 _mapping_
[source,js]
----
GET /gb/_mapping/tweet
----

返回数据如下：
[source,js]
----
{
   "gb": {
      "mappings": {
         "tweet": {
            "properties": {
               "date": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "name": {
                  "type": "string"
               },
               "tweet": {
                  "type": "string"
               },
               "user_id": {
                  "type": "long"
               }
            }
         }
      }
   }
}
----

`_all` 虽然在上面的 mapping 中没有体现出来，但是我们知道 `_all` 是存在的，且类型为 string

[TIP]
====
_ES_ 会根据数据的类型动态创建对应的 _Mapping_ 。
====

到目前为止，上述问题最主要的区别在于 _ES_ 中的两种字段类型，精确值类型与全文类型。

== 精确值类型与全文类型比较
在 _ES_ 中，类型可以被粗分为两种类型：精确值类型、全文值类型

精确值类型::
精确值类型在索引时是什么样的，查询时匹配也必须是什么样的。

全文值类型::
全文值类型索引时会分词，查询时对查询关键字也会分词。


[TIP]
====
全文文本常常被称为 `非结构化数据` ，其实是一种用词不当的称谓，实际上自然语言是高度结构化的。问题是自然语言的语法规则是如此的复杂，计算机难以正确解析。例如这个句子：

[source,js]
----
May is fun but June bores me.
----
到底是说的月份还是人呢？
====

精确值倒是非常容易匹配，因为匹配时要么是匹配上，要么是匹配不上。这种匹配很容易被解释为 _SQL_

[source,sql]
----
WHERE name    = "John Smith"
  AND user_id = 2
  AND date    > "2014-09-15"
----

而对于全文数据的查询来说，却有些微妙。我们不会去询问`这篇文档是否匹配查询要求？`。 但是，我们会询问这篇文档和查询的匹配程度如何？。换句话说，对于查询条件，`这篇文档的相关性有多高？`

我们很少确切的匹配整个全文文本。我们想在全文中查询包含查询文本的部分。不仅如此，我们还期望搜索引擎能理解我们的意图：

* 一个针对"UK"的查询将返回涉及"United Kingdom"的文档
* 一个针对"jump"的查询同时能够匹配"jumped"， "jumps"， "jumping"甚至"leap"
* "johnny walker"也能匹配"Johnnie Walker"， "johnnie depp"及"Johnny Depp"+
* "fox news hunting"能返回有关hunting on Fox News的故事，而"fox hunting news"也能返回关于fox hunting的新闻故事。

=== 倒排索引
_ES_ 使用一个数据结构，称为 _inverted index_ 。倒排索引由在文档中出现的唯一的词列表，以及对于每个词在文档中的位置组成。

例如，我们有两个文档，每个文档content字段包含：

. The quick brown fox jumped over the lazy dog
. Quick brown foxes leap over lazy dogs in summer

为了创建倒排索引，首先需要将 _content_ 字段拆分为单独的词，然后将所有的词放入列表并排序。


|===
|Term   |   Doc_1|  Doc_2
|Quick   |       |  X
|The     |   X   |
|brown   |   X   |  X
|dog     |   X   |
|dogs    |       |  X
|fox     |   X   |
|foxes   |       |  X
|in      |       |  X
|jumped  |   X   |
|lazy    |   X   |  X
|leap    |       |  X
|over    |   X   |  X
|quick   |   X   |
|summer  |       |  X
|the     |   X   |
|===

现在，如何需要搜索 `quick brown` ,我们只需要找到出现了该词的文档。

|===
|Term    | Doc_1 |Doc_2
|brown   |   X   |  X
|quick   |   X   |
|Total   |   2   |  1
|===

这两个文档都匹配上了，然而第一个匹配度更高一些。如果我们只简单地根据匹配的词数量来决定相关度，那么明显第一个比第二个的匹配度更高。

.但是现在的这个倒排索引存在以下几个问题：
. "Quick"和"quick"被认为是不同的单词，但是用户可能认为它们是相同的。
. "fox"和"foxes"很相似，就像"dog"和"dogs"——它们都是同根词。
. "jumped"和"leap"不是同根词，但意思相似——它们是同义词。


如果我们将词为统一为标准格式，这样就可以找到不是确切匹配查询，但是足以相似从而可以关联的文档。例如：

. "Quick"可以转为小写成为"quick"。
. "foxes"可以被转为根形式"fox"。同理"dogs"可以被转为"dog"。
. "jumped"和"leap"同义就可以只索引为单个词"jump"

此时倒排索引就像下面这样
|===
|Term	|Doc_1	|Doc_2
|brown	|X		|X
|dog	|X		|X
|fox	|X		|X
|in		|X  	|
|jump	|X		|X
|lazy	|X		|X
|over	|X		|X
|quick	|X		|X
|summer	|		|X
|the	|X		|X
|===

如果此时我们以 _Quick_ 搜索还是会失败，因为 _Quick_ 确实不在词表中。这个时候就需要对搜索的词处理成标准格式。

[IMPORTANT]
====
这很重要。你只可以找到确实存在于索引中的词，所以索引文本和查询字符串都要标准化为相同的形式。
====

这种分词和标准化的过程我们称之为 _analysis_ ，即分析。

== 分析与分析器
分析过程由以下几个步骤组成：

. 将文本拆分为词，并用于倒排索引
. 然后将词处理成标准格式，以提高其搜索性

上面说的这个工作由分析器来完成。分析器实际上由下面的三块功能组成：

字符过滤器::
首先字符串经过字符过滤器(character filter)，它们的工作是在标记化前处理字符串。字符过滤器能够去除HTML标记，或者转换"&"为"and"。

分词器::
下一步，分词器(tokenizer)被标记化成独立的词。一个简单的分词器(tokenizer)可以根据空格或逗号将单词分开

词过滤::
最后，每个词都通过所有标记过滤(token filters)，它可以修改词（例如将"Quick"转为小写），去掉词（例如停用词像"a"、"and"、"the"等等），或者增加词（例如同义词像"jump"和"leap"）

=== 内置的分析器
_ElasticSearch_ 预置了一些分析器，你可以直接使用。下面列出几个比较重要的分析器，演示这个字符串分词后的差异：

----
"Set the shape to semi-transparent by calling set_trans(5)"
----

标准分析器::
标准分析器是Elasticsearch默认使用的分析器。对于文本分析，它对于任何语言都是最佳选择（译者注：就是没啥特殊需求，对于任何一个国家的语言，这个分析器就够用了）。它根据Unicode Consortium的定义的单词边界(word boundaries)来切分文本，然后去掉大部分标点符号。最后，把所有词转为小写。产生的结果为：
+
----
set, the, shape, to, semi, transparent, by, calling, set_trans, 5
----

简单分析器::
简单分析器将非单个字母的文本切分，然后把每个词转为小写。产生的结果为：
+
----
set, the, shape, to, semi, transparent, by, calling, set, trans
----

空格分析器::
空格分析器依据空格切分文本。它不转换小写。产生结果为：
+
----
Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
----

语言分析器::
特定语言分析器适用于很多语言。它们能够考虑到特定语言的特性。例如，english分析器自带一套英语停用词库——像and或the这些与语义无关的通用词。这些词被移除后，因为语法规则的存在，英语单词的主体含义依旧能被理解（译者注：stem English words这句不知道该如何翻译，查了字典，我理解的大概意思应该是将英语语句比作一株植物，去掉无用的枝叶，主干依旧存在，停用词好比枝叶，存在与否并不影响对这句话的理解。）。
+
english分析器将会产生以下结果：
+
----
set, shape, semi, transpar, call, set_tran, 5
----

=== 当分析器使用时
当我们索引(index)一个文档，全文字段会被分析为单独的词来创建倒排索引。不过，当我们在全文字段搜索(search)时，我们要让查询字符串经过同样的分析流程处理，以确保这些词在索引中存在。

全文查询我们将在稍后讨论，理解每个字段是如何定义的，这样才可以让它们做正确的事：
. 当你查询全文(full text)字段，查询将使用相同的分析器来分析查询字符串，以产生正确的词列表。
. 当你查询一个确切值(exact value)字段，查询将不分析查询字符串，但是你可以自己指定。

现在你可以明白为什么《映射和分析》的开头会产生那种结果:

. date字段包含一个确切值：单独的一个词"2014-09-15"。
. _all字段是一个全文字段，所以分析过程将日期转为三个词："2014"、"09"和"15"。

当我们在_all字段查询2014，它一个匹配到12条推文，因为这些推文都包含词2014：

[source,js]
----
GET /_search?q=2014              # 12 results
----

当我们在_all字段中查询2014-09-15，首先分析查询字符串，产生匹配任一词2014、09或15的查询语句，它依旧匹配12个推文，因为它们都包含词2014。

[source,js]
----
GET /_search?q=2014-09-15        # 12 results !
----

当我们在date字段中查询2014-09-15，它查询一个确切的日期，然后只找到一条推文：

[source,js]
----
GET /_search?q=date:2014-09-15   # 1  result
----

当我们在date字段中查询2014，没有找到文档，因为没有文档包含那个确切的日期：
[source,js]
----
GET /_search?q=date:2014         # 0  results !
----

=== 测试分析器
当你新接触 _ES_ 时，通常比较难理解文本是如何分词并索引的。为了更好地理解分析器，可以使用 _analyze api_ 来查看文本是否被分词的。

[source,js]
----
GET /_analyze
{
  "analyzer": "standard",  // <1>
  "text": "Text to analyze" // <2>
}
----
<1> 指定选用的分析器
<2> 指定要分析的文本

token是一个实际被存储在索引中的词。position指明词在原文本中是第几个出现的。start_offset和end_offset表示词在原文本中占据的位置

=== 指定使用的分析器
当Elasticsearch在你的文档中探测到一个新的字符串字段，它将自动设置它为全文string字段并用standard分析器分析。

你不可能总是想要这样做。也许你想使用一个更适合这个数据的语言分析器。或者，你只想把字符串字段当作一个普通的字段——不做任何分析，只存储确切值，就像字符串类型的用户ID或者内部状态字段或者标签。

为了达到这种效果，我们必须通过映射(mapping)人工设置这些字段。

== 映射

为了能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理成全文本（Full-text）或精确的字符串值，Elasticsearch需要知道每个字段里面都包含了什么类型。这些类型和字段的信息存储（包含）在映射（mapping）中。

正如《Data In, Data Out》一节所说，索引中每个文档都有一个类型(type)。 每个类型拥有自己的映射(mapping)或者模式定义(schema definition)。一个映射定义了字段类型，每个字段的数据类型，以及字段被Elasticsearch处理的方式。映射还用于设置关联到类型上的元数据。

=== 核心简单字段类型
Elasticsearch支持以下简单字段类型：

|===
|类型				|表示的数据类型
|String				|string
|Whole number		|byte, short, integer, long
|Floating point		|float, double
|Boolean			|boolean
|Date				|date
|===

当你索引一个包含新字段的文档——一个之前没有的字段——Elasticsearch将使用动态映射猜测字段类型，这类型来自于JSON的基本数据类型，使用以下规则：

|===
|JSON type							|Field type
|Boolean: true or false				|"boolean"
|Whole number: 123					|"long"
|Floating point: 123.45				|"double"
|String, valid date: "2014-09-15"	|"date"
|String: "foo bar"					|"string"
|===

[TIP]
====
这意味着，如果你索引一个带引号的数字——"123"，它将被映射为"string"类型，而不是"long"类型。

然而，如果字段已经被映射为"long"类型，你再存储"123"时，Elasticsearch将尝试转换字符串为long，并在转换失败时会抛出异常。
====


=== 查看映射

[source,js]
----
GET /gb/_mapping/tweet
----

[source,js]
----
{
   "gb": {
      "mappings": {
         "tweet": {
            "properties": {
               "date": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "name": {
                  "type": "string"
               },
               "tweet": {
                  "type": "string"
               },
               "user_id": {
                  "type": "long"
               }
            }
         }
      }
   }
}
----

[TIP]
====
当映射错误时，如将 _age_ 字段用 _string_ 类型而非 _integer_ 类型表示时，这可能会导致你的查询结果令人疑惑。

不要假设你的类型映射是正确的，你需要动手检查它。
====

=== 自定义字段映射
虽然大多数情况下基本数据类型已经能够满足，但你也会经常需要自定义一些特殊类型（fields），特别是字符串字段类型。

自定义类型可以使你完成一下几点：

* 区分全文（full text）字符串字段和准确字符串字段（译者注：就是分词与不分词，全文的一般要分词，准确的就不需要分词，比如『中国』这个词。全文会分成『中』和『国』，但作为一个国家标识的时候我们是不需要分词的，所以它就应该是一个准确的字符串字段）。

* 使用特定语言的分析器（译者注：例如中文、英文、阿拉伯语，不同文字的断字、断词方式的差异）

* 优化部分匹配字段

* 指定自定义日期格式（译者注：这个比较好理解,例如英文的 `Feb,12,2016` 和 中文的 `2016年2月12日`）

* 以及更多

映射中最重要的字段参数是`type`。除了`string`类型的字段，你可能很少需要映射其他的`type`：

```javascript

{

    "number_of_clicks": {

        "type": "integer"

    }

}

```

`string`类型的字段，默认的，考虑到包含全文本，它们的值在索引前要经过分析器分析，并且在全文搜索此字段前要把查询语句做分析处理。

对于`string`字段，两个最重要的映射参数是`index`和`analyer`。


==== index
`index`参数控制字符串以何种方式被索引。它包含以下三个值当中的一个：

analyzed::
首先分析这个字符串，然后索引。换言之，以全文形式索引此字段。（分词）

not_analyzed::
索引这个字段，使之可以被搜索，但是索引内容和指定值一样。不分析此字段。（不分词）

no::
不索引此字段，这个字段不能被搜索到。

_string_ 类型默认的 _index_ 值为 `analyzed`，如果我们需要某一 _string_ 类型作为精确值查询，需要设置其 _index_ 值为 `not_analyzed`

[source,json]
----
{
    "tag": {
        "type":     "string",
        "index":    "not_analyzed"
    }
}
----

[NOTE]
====
其他简单类型（`long`、`double`、`date`等等）也接受`index`参数，但相应的值只能是`no`和`not_analyzed`，它们的值不能被分析。
====

===== analyzer
对于`analyzed`类型的字符串字段，使用`analyzer`参数来指定哪一种分析器将在搜索和索引的时候使用。默认的，Elasticsearch使用`standard`分析器，但是你可以通过指定一个内建的分析器来更改它，例如`whitespace`、`simple`或`english`。

```javascript

{

    "tweet": {

        "type":     "string",

        "analyzer": "english"

    }

}

```

=== 更新映射
你可以在第一次创建索引的时候指定映射的类型。此外，你也可以晚些时候为新类型添加映射（或者为已有的类型更新映射）。

[IMPORTANT]
====
你可以向已有映射中 *增加* 字段，但你不能 *修改* 它。如果一个字段在映射中已经存在，这可能意味着那个字段的数据已经被索引。如果你改变了字段映射，那已经被索引的数据将错误并且不能被正确的搜索到。
====

我们可以更新一个映射来增加一个新字段，但是不能把已有字段的类型那个从`analyzed`改到`not_analyzed`。

为了演示两个指定的映射方法，让我们首先删除索引`gb`：

```sh

DELETE /gb

```

然后创建一个新索引，指定`tweet`字段的分析器为`english`：

```javascript

PUT /gb <1>

{

  "mappings": {

    "tweet" : {

      "properties" : {

        "tweet" : {

          "type" :    "string",

          "analyzer": "english"

        },

        "date" : {

          "type" :   "date"

        },

        "name" : {

          "type" :   "string"

        },

        "user_id" : {

          "type" :   "long"

        }

      }

    }

  }

}

```

<1> 这将创建包含`mappings`的索引，映射在请求体中指定。

再后来，我们决定在`tweet`的映射中增加一个新的`not_analyzed`类型的文本字段，叫做`tag`，使用`_mapping`后缀:

```javascript

PUT /gb/_mapping/tweet

{

  "properties" : {

    "tag" : {

      "type" :    "string",

      "index":    "not_analyzed"

    }

  }

}

```

注意到我们不再需要列出所有的已经存在的字段，因为我们没法修改他们。我们的新字段已经被合并至存在的那个映射中。

=== 测试映射

你可以通过名字使用`analyze` API测试字符串字段的映射。对比这两个请求的输出：

```javascript

GET /gb/_analyze?field=tweet&text=Black-cats <1>

GET /gb/_analyze?field=tag&text=Black-cats

```

<1> 我们想要分析的文本被放在请求体中。

`tweet`字段产生两个词，`"black"`和`"cat"`,`tag`字段产生单独的一个词`"Black-cats"`。换言之，我们的映射工作正常。

== 复合类型
除了之前提到的简单的标量类型，JSON还有`null`值，数组和对象，所有这些Elasticsearch都支持：

=== 多值字段

我们想让`tag`字段包含多个字段，这非常有可能发生。我们可以索引一个标签数组来代替单一字符串：

```javascript

{ "tag": [ "search", "nosql" ]}

```

对于数组不需要特殊的映射。任何一个字段可以包含零个、一个或多个值，同样对于全文字段将被分析并产生多个词。

言外之意，这意味着**数组中所有值必须为同一类型**。你不能把日期和字符窜混合。如果你创建一个新字段，这个字段索引了一个数组，Elasticsearch将使用第一个值的类型来确定这个新字段的类型。

[NOTE]
====
当你从Elasticsearch中取回一个文档，任何一个数组的顺序和你索引它们的顺序一致。你取回的`_source`字段的顺序同样与索引它们的顺序相同。

然而，数组是做为多值字段被**索引**的，它们没有顺序。在搜索阶段你不能指定“第一个值”或者“最后一个值”。倒不如把数组当作一个**值集合(bag of values)**
====

=== 空字段
当然数组可以是空的。这等价于有零个值。事实上，Lucene没法存放`null`值，所以一个`null`值的字段被认为是空字段。

这四个字段将被识别为空字段而 *不被索引* ：

```javascript

"empty_string":             "",

"null_value":               null,

"empty_array":              [],

"array_with_null_value":    [ null ]

```

=== 多级（嵌套）对象

我们需要讨论的最后一个自然JSON数据类型是**对象(object)**——在其它语言中叫做hash、hashmap、dictionary 或者 associative array.

**内部对象(inner objects)**经常用于在另一个对象中嵌入一个实体或对象。例如，做为在`tweet`文档中`user_name`和`user_id`的替代，我们可以这样写：

```javascript

{

    "tweet":            "Elasticsearch is very flexible",

    "user": {

        "id":           "@johnsmith",

        "gender":       "male",

        "age":          26,

        "name": {

            "full":     "John Smith",

            "first":    "John",

            "last":     "Smith"

        }

    }

}

```

=== 嵌套对象的映射

Elasticsearch 会动态的检测新对象的字段，并且映射它们为 `object` 类型，将每个字段加到 `properties` 字段下

[source,js]
----

{

  "gb": {

    "tweet": { <1>

      "properties": {

        "tweet":            { "type": "string" },

        "user": { <2>

          "type":             "object",

          "properties": {

            "id":           { "type": "string" },

            "gender":       { "type": "string" },

            "age":          { "type": "long"   },

            "name":   { <3>

              "type":         "object",

              "properties": {

                "full":     { "type": "string" },

                "first":    { "type": "string" },

                "last":     { "type": "string" }

              }

            }

          }

        }

      }

    }

  }

}
----
<1> 根对象.
<2> 内部对象
<3> 内部对象.


对`user`和`name`字段的映射与`tweet`类型自己很相似。事实上，`type`映射只是`object`映射的一种特殊类型，我们将 `object` 称为_根对象_。它与其他对象一模一样，除非它有一些特殊的顶层字段，比如 `_source`, `_all` 等等。


=== 嵌套对象是怎样被索引的

Lucene 并不了解内部对象。 一个 Lucene 文件包含一个 *键-值对应的扁平表单* 。 为了让 Elasticsearch 可以有效的索引内部对象，将文件转换为以下格式：

```javascript
{
    "tweet":            [elasticsearch, flexible, very],
    "user.id":          [@johnsmith],
    "user.gender":      [male],
    "user.age":         [26],
    "user.name.full":   [john, smith],
    "user.name.first":  [john],
    "user.name.last":   [smith]
}
```

_内部栏位_可被归类至name，例如`"first"`。 为了区别两个拥有相同名字的栏位，我们可以使用完整_路径_，例如`"user.name.first"` 或甚至`类型`名称加上路径：`"tweet.user.name.first"`。

[NOTE]
====
注意： 在以上扁平化文件中，并没有栏位叫作`user`也没有栏位叫作`user.name`。 Lucene 只索引阶层或简单的值，而不会索引复杂的资料结构。
====

=== 嵌套对象数组

最后，一个包含内部对象的数组如何索引。 我们有个数组如下所示：

[source,js]
----
{
    "followers": [
        { "age": 35, "name": "Mary White"},
        { "age": 26, "name": "Alex Jones"},
        { "age": 19, "name": "Lisa Smith"}
    ]
}
----

然而在 _Lucene_ 中，上述文件会被扁平化存储，最后可能会像下面这样：

[source,js]
----
{
    "followers.age":    [19, 26, 35],
    "followers.name":   [alex, jones, lisa, smith, mary, white]
}
----

`{age: 35}`与`{name: Mary White}`之间的关联会消失，因每个多值的栏位会变成一个值集合，而非有序的阵列。 这让我们可以知道：

* _是否有26岁的追随者？_

但我们无法取得准确的资料如：

* _是否有26岁的追随者且名字叫Alex Jones？_

关联内部对象可解决此类问题，我们称之为_嵌套_对象，我们之後会在嵌套对象中提到它。
